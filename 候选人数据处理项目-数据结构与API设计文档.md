# ğŸ“‹ æ•°æ®ç»“æ„ä¸APIè®¾è®¡æ–‡æ¡£ - å€™é€‰äººæ•°æ®å¤„ç†é¡¹ç›®

## ğŸ—ï¸ **é¡¹ç›®æ¶æ„å»ºè®®**

```
crawler_project/                    # çˆ¬è™«æ•°æ®å¤„ç†é¡¹ç›®
â”œâ”€â”€ ğŸ“ crawler/                     # çˆ¬è™«æ¨¡å—
â”‚   â”œâ”€â”€ spiders/                    # çˆ¬è™«è„šæœ¬
â”‚   â”œâ”€â”€ items.py                    # æ•°æ®é¡¹å®šä¹‰
â”‚   â”œâ”€â”€ pipelines.py                # æ•°æ®å¤„ç†ç®¡é“
â”‚   â””â”€â”€ settings.py                 # çˆ¬è™«é…ç½®
â”œâ”€â”€ ğŸ“ processor/                   # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ normalizer.py               # æ•°æ®æ ‡å‡†åŒ–
â”‚   â”œâ”€â”€ validator.py                # æ•°æ®éªŒè¯
â”‚   â””â”€â”€ formatter.py                # æ•°æ®æ ¼å¼åŒ–
â”œâ”€â”€ ğŸ“ api/                         # APIæ¥å£æ¨¡å—
â”‚   â”œâ”€â”€ models.py                   # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ views.py                    # APIè§†å›¾
â”‚   â””â”€â”€ serializers.py              # æ•°æ®åºåˆ—åŒ–
â”œâ”€â”€ ğŸ“ output/                      # è¾“å‡ºæ¨¡å—
â”‚   â”œâ”€â”€ json_exporter.py            # JSONå¯¼å‡º
â”‚   â”œâ”€â”€ sql_generator.py            # SQLç”Ÿæˆ
â”‚   â””â”€â”€ api_sender.py               # APIå‘é€
â””â”€â”€ ğŸ“„ requirements.txt             # é¡¹ç›®ä¾èµ–
```

---

## ğŸ—„ï¸ **ç›®æ ‡ç³»ç»Ÿæ•°æ®ç»“æ„**

### 1ï¸âƒ£ **å€™é€‰äººæ ¸å¿ƒæ•°æ®ç»“æ„**

#### ğŸ“Š **æ•°æ®åº“è¡¨ç»“æ„**

```sql
-- ===============================================
-- å€™é€‰äººä¿¡æ¯è¡¨ (jobs_candidate)
-- ===============================================
CREATE TABLE `jobs_candidate` (
  -- åŸºç¡€å­—æ®µ
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'ä¸»é”®ID',
  `name` varchar(100) NOT NULL COMMENT 'å€™é€‰äººå§“å',
  `email` varchar(254) DEFAULT NULL COMMENT 'é‚®ç®±åœ°å€',
  `phone` varchar(20) DEFAULT NULL COMMENT 'æ‰‹æœºå·ç ',
  
  -- èŒä¸šä¿¡æ¯
  `current_position` varchar(200) DEFAULT NULL COMMENT 'å½“å‰èŒä½',
  `current_company` varchar(200) DEFAULT NULL COMMENT 'å½“å‰å…¬å¸',
  `experience_years` int(11) DEFAULT NULL COMMENT 'å·¥ä½œå¹´é™(æ•´æ•°)',
  `education` varchar(200) DEFAULT NULL COMMENT 'æœ€é«˜å­¦å†',
  
  -- æŠ€èƒ½å’Œèƒ½åŠ›
  `skills` longtext DEFAULT NULL COMMENT 'æŠ€èƒ½æ ‡ç­¾(JSONæ•°ç»„æ ¼å¼)',
  `predicted_job_tags` longtext DEFAULT NULL COMMENT 'AIé¢„æµ‹é€‚åˆèŒä½æ ‡ç­¾(JSONæ•°ç»„)',
  
  -- ç®€å†ç›¸å…³
  `resume_file` varchar(100) DEFAULT NULL COMMENT 'ç®€å†æ–‡ä»¶è·¯å¾„',
  `resume_content` longtext DEFAULT NULL COMMENT 'ç®€å†æ–‡æœ¬å†…å®¹',
  `ai_parsed_data` longtext DEFAULT NULL COMMENT 'AIè§£æçš„ç»“æ„åŒ–æ•°æ®(JSON)',
  
  -- æ±‚èŒæ„å‘
  `salary_expectation` varchar(100) DEFAULT NULL COMMENT 'æœŸæœ›è–ªèµ„',
  `location_preference` varchar(200) DEFAULT NULL COMMENT 'æœŸæœ›å·¥ä½œåœ°ç‚¹',
  `availability` varchar(50) DEFAULT NULL COMMENT 'å¯å…¥èŒæ—¶é—´',
  
  -- ç®¡ç†å­—æ®µ
  `notes` longtext DEFAULT NULL COMMENT 'å¤‡æ³¨ä¿¡æ¯',
  `status` varchar(20) DEFAULT 'active' COMMENT 'çŠ¶æ€(active/inactive/hired/blacklist)',
  `source` varchar(100) DEFAULT NULL COMMENT 'æ•°æ®æ¥æºæ¸ é“',
  `user_id` bigint(20) NOT NULL COMMENT 'æ‰€å±ç”¨æˆ·ID',
  
  -- æ—¶é—´æˆ³
  `created_at` datetime(6) NOT NULL COMMENT 'åˆ›å»ºæ—¶é—´',
  `updated_at` datetime(6) NOT NULL COMMENT 'æ›´æ–°æ—¶é—´',
  
  -- ç´¢å¼•
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_email_user` (`email`, `user_id`),
  KEY `idx_phone` (`phone`),
  KEY `idx_status` (`status`),
  KEY `idx_created_at` (`created_at`),
  KEY `idx_user_id` (`user_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='å€™é€‰äººä¿¡æ¯è¡¨';

-- ===============================================
-- å€™é€‰äººåˆ†ç»„è¡¨ (jobs_candidategroup)
-- ===============================================
CREATE TABLE `jobs_candidategroup` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'åˆ†ç»„ID',
  `name` varchar(100) NOT NULL COMMENT 'åˆ†ç»„åç§°',
  `description` longtext DEFAULT NULL COMMENT 'åˆ†ç»„æè¿°',
  `color` varchar(20) DEFAULT '#007bff' COMMENT 'åˆ†ç»„é¢œè‰²ä»£ç ',
  `tags` longtext DEFAULT NULL COMMENT 'åˆ†ç»„æ ‡ç­¾(JSONæ•°ç»„)',
  `user_id` bigint(20) NOT NULL COMMENT 'æ‰€å±ç”¨æˆ·ID',
  `created_at` datetime(6) NOT NULL COMMENT 'åˆ›å»ºæ—¶é—´',
  `updated_at` datetime(6) NOT NULL COMMENT 'æ›´æ–°æ—¶é—´',
  
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_name_user` (`name`, `user_id`),
  KEY `idx_user_id` (`user_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='å€™é€‰äººåˆ†ç»„è¡¨';

-- ===============================================
-- å€™é€‰äººåˆ†ç»„å…³è”è¡¨ (jobs_candidate_groups)
-- ===============================================
CREATE TABLE `jobs_candidate_groups` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `candidate_id` bigint(20) NOT NULL COMMENT 'å€™é€‰äººID',
  `candidategroup_id` bigint(20) NOT NULL COMMENT 'åˆ†ç»„ID',
  
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_candidate_group` (`candidate_id`, `candidategroup_id`),
  KEY `idx_candidate_id` (`candidate_id`),
  KEY `idx_group_id` (`candidategroup_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='å€™é€‰äººåˆ†ç»„å…³è”è¡¨';
```

### 2ï¸âƒ£ **JSONæ•°æ®æ ¼å¼è§„èŒƒ**

#### ğŸ·ï¸ **æŠ€èƒ½æ ‡ç­¾æ ¼å¼ (skills)**
```json
{
  "programming_languages": ["Python", "Java", "JavaScript", "Go"],
  "frameworks": ["Django", "Spring Boot", "React", "Vue.js"],
  "databases": ["MySQL", "Redis", "MongoDB", "PostgreSQL"],
  "tools": ["Git", "Docker", "Kubernetes", "Jenkins"],
  "soft_skills": ["å›¢é˜Ÿåä½œ", "é¡¹ç›®ç®¡ç†", "æ²Ÿé€šèƒ½åŠ›"],
  "certifications": ["AWSè®¤è¯", "PMPè®¤è¯"],
  "languages": [
    {"language": "ä¸­æ–‡", "level": "æ¯è¯­"},
    {"language": "è‹±è¯­", "level": "ç†Ÿç»ƒ"}
  ]
}
```

#### ğŸ¯ **AIé¢„æµ‹èŒä½æ ‡ç­¾æ ¼å¼ (predicted_job_tags)**
```json
{
  "primary_roles": [
    {"role": "åç«¯å¼€å‘å·¥ç¨‹å¸ˆ", "confidence": 0.95},
    {"role": "å…¨æ ˆå¼€å‘å·¥ç¨‹å¸ˆ", "confidence": 0.85}
  ],
  "secondary_roles": [
    {"role": "æŠ€æœ¯ç»ç†", "confidence": 0.70},
    {"role": "æ¶æ„å¸ˆ", "confidence": 0.65}
  ],
  "industries": ["äº’è”ç½‘", "é‡‘èç§‘æŠ€", "ç”µå•†"],
  "seniority_level": "ä¸­çº§", // åˆçº§/ä¸­çº§/é«˜çº§/ä¸“å®¶
  "job_categories": ["æŠ€æœ¯", "ç ”å‘", "åç«¯"]
}
```

#### ğŸ¤– **AIè§£ææ•°æ®æ ¼å¼ (ai_parsed_data)**
```json
{
  "extraction_info": {
    "model_used": "qwen-plus",
    "confidence": 0.92,
    "extraction_time": "2024-01-15T10:30:00Z",
    "source_type": "resume_pdf"
  },
  "personal_info": {
    "full_name": "å¼ ä¸‰",
    "gender": "ç”·",
    "age": 28,
    "birth_year": 1995,
    "marital_status": "æœªå©š"
  },
  "contact_info": {
    "emails": ["zhangsan@example.com"],
    "phones": ["13800138000"],
    "location": "åŒ—äº¬å¸‚æœé˜³åŒº",
    "address": "å…·ä½“åœ°å€ä¿¡æ¯"
  },
  "education": [
    {
      "school": "æ¸…åå¤§å­¦",
      "degree": "æœ¬ç§‘",
      "major": "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯",
      "start_date": "2013-09",
      "end_date": "2017-07",
      "gpa": "3.8/4.0"
    }
  ],
  "work_experience": [
    {
      "company": "å­—èŠ‚è·³åŠ¨",
      "position": "é«˜çº§åç«¯å¼€å‘å·¥ç¨‹å¸ˆ",
      "start_date": "2020-03",
      "end_date": "2024-01",
      "duration": "3å¹´10ä¸ªæœˆ",
      "responsibilities": [
        "è´Ÿè´£æ¨èç³»ç»Ÿåç«¯å¼€å‘",
        "ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œæå‡QPS 50%",
        "å¸¦é¢†3äººå°å›¢é˜Ÿå®Œæˆé¡¹ç›®"
      ],
      "technologies": ["Python", "Django", "MySQL", "Redis"]
    }
  ],
  "projects": [
    {
      "name": "ç”¨æˆ·æ¨èç³»ç»Ÿ",
      "role": "æ ¸å¿ƒå¼€å‘",
      "duration": "6ä¸ªæœˆ",
      "description": "æ„å»ºåƒä¸‡çº§ç”¨æˆ·æ¨èç³»ç»Ÿ",
      "technologies": ["Python", "TensorFlow", "Redis"],
      "achievements": ["æå‡ç‚¹å‡»ç‡15%", "æ—¥å¤„ç†è¯·æ±‚1äº¿æ¬¡"]
    }
  ],
  "skills_analysis": {
    "technical_skills": {
      "programming": ["Python", "Java", "JavaScript"],
      "frameworks": ["Django", "Spring"],
      "databases": ["MySQL", "Redis"],
      "proficiency_level": "é«˜çº§"
    },
    "years_experience": {
      "total": 6,
      "python": 5,
      "backend": 6,
      "team_lead": 2
    }
  },
  "salary_info": {
    "current_salary": "å¹´è–ª40ä¸‡",
    "expected_salary": "å¹´è–ª50-60ä¸‡",
    "currency": "CNY"
  }
}
```

---

## ğŸ“ **æ•°æ®å¤„ç†é¡¹ç›®ä»£ç ç»“æ„**

### 1ï¸âƒ£ **æ•°æ®é¡¹å®šä¹‰ (items.py)**

```python
import scrapy
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from datetime import datetime

@dataclass
class CandidateItem:
    """å€™é€‰äººæ•°æ®é¡¹"""
    
    # åŸºç¡€ä¿¡æ¯
    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    
    # èŒä¸šä¿¡æ¯  
    current_position: Optional[str] = None
    current_company: Optional[str] = None
    experience_years: Optional[int] = None
    education: Optional[str] = None
    
    # æŠ€èƒ½å’Œæ ‡ç­¾
    skills: List[str] = None
    predicted_job_tags: List[str] = None
    
    # ç®€å†ç›¸å…³
    resume_file: Optional[str] = None
    resume_content: Optional[str] = None
    ai_parsed_data: Optional[Dict[str, Any]] = None
    
    # æ±‚èŒæ„å‘
    salary_expectation: Optional[str] = None
    location_preference: Optional[str] = None
    availability: Optional[str] = None
    
    # ç®¡ç†å­—æ®µ
    notes: Optional[str] = None
    status: str = 'active'
    source: Optional[str] = None
    
    # æ—¶é—´æˆ³
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    
    # åŸå§‹æ•°æ®
    raw_data: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'name': self.name,
            'email': self.email,
            'phone': self.phone,
            'current_position': self.current_position,
            'current_company': self.current_company,
            'experience_years': self.experience_years,
            'education': self.education,
            'skills': self.format_skills(),
            'predicted_job_tags': self.format_job_tags(),
            'resume_file': self.resume_file,
            'resume_content': self.resume_content,
            'ai_parsed_data': self.format_ai_data(),
            'salary_expectation': self.salary_expectation,
            'location_preference': self.location_preference,
            'availability': self.availability,
            'notes': self.notes,
            'status': self.status,
            'source': self.source,
            'created_at': self.created_at or datetime.now(),
            'updated_at': datetime.now()
        }
    
    def format_skills(self) -> str:
        """æ ¼å¼åŒ–æŠ€èƒ½ä¸ºJSONå­—ç¬¦ä¸²"""
        if not self.skills:
            return '[]'
        import json
        return json.dumps(self.skills, ensure_ascii=False)
    
    def format_job_tags(self) -> str:
        """æ ¼å¼åŒ–èŒä½æ ‡ç­¾ä¸ºJSONå­—ç¬¦ä¸²"""
        if not self.predicted_job_tags:
            return '[]'
        import json
        return json.dumps(self.predicted_job_tags, ensure_ascii=False)
    
    def format_ai_data(self) -> str:
        """æ ¼å¼åŒ–AIæ•°æ®ä¸ºJSONå­—ç¬¦ä¸²"""
        if not self.ai_parsed_data:
            return '{}'
        import json
        return json.dumps(self.ai_parsed_data, ensure_ascii=False)
```

### 2ï¸âƒ£ **æ•°æ®å¤„ç†ç®¡é“ (pipelines.py)**

```python
import json
import mysql.connector
from datetime import datetime
import logging
from typing import Dict, Any

class DataValidationPipeline:
    """æ•°æ®éªŒè¯ç®¡é“"""
    
    def process_item(self, item: CandidateItem, spider):
        """éªŒè¯æ•°æ®é¡¹"""
        
        # å¿…å¡«å­—æ®µéªŒè¯
        if not item.name or not item.name.strip():
            raise ValueError("å€™é€‰äººå§“åä¸èƒ½ä¸ºç©º")
        
        # é‚®ç®±æ ¼å¼éªŒè¯
        if item.email:
            import re
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            if not re.match(email_pattern, item.email):
                spider.logger.warning(f"é‚®ç®±æ ¼å¼ä¸æ­£ç¡®: {item.email}")
                item.email = None
        
        # æ‰‹æœºå·æ ¼å¼éªŒè¯
        if item.phone:
            phone_clean = re.sub(r'[^\d]', '', item.phone)
            if len(phone_clean) == 11 and phone_clean.startswith('1'):
                item.phone = phone_clean
            else:
                spider.logger.warning(f"æ‰‹æœºå·æ ¼å¼ä¸æ­£ç¡®: {item.phone}")
                item.phone = None
        
        # å·¥ä½œå¹´é™éªŒè¯
        if item.experience_years is not None:
            if not isinstance(item.experience_years, int) or item.experience_years < 0 or item.experience_years > 50:
                spider.logger.warning(f"å·¥ä½œå¹´é™ä¸åˆç†: {item.experience_years}")
                item.experience_years = None
        
        return item

class DataNormalizationPipeline:
    """æ•°æ®æ ‡å‡†åŒ–ç®¡é“"""
    
    def process_item(self, item: CandidateItem, spider):
        """æ ‡å‡†åŒ–æ•°æ®"""
        
        # å§“åæ ‡å‡†åŒ–
        if item.name:
            item.name = item.name.strip()
        
        # å…¬å¸åç§°æ ‡å‡†åŒ–
        if item.current_company:
            # ç§»é™¤å¸¸è§çš„å…¬å¸åç¼€å˜ä½“
            company_suffixes = ['æœ‰é™å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸', 'ç§‘æŠ€æœ‰é™å…¬å¸', 'Ltd.', 'Inc.', 'Corp.']
            for suffix in company_suffixes:
                if item.current_company.endswith(suffix):
                    item.current_company = item.current_company[:-len(suffix)].strip()
                    break
        
        # èŒä½åç§°æ ‡å‡†åŒ–
        if item.current_position:
            # æ ‡å‡†åŒ–å¸¸è§èŒä½åç§°
            position_mapping = {
                'è½¯ä»¶å¼€å‘å·¥ç¨‹å¸ˆ': 'è½¯ä»¶å·¥ç¨‹å¸ˆ',
                'ç¨‹åºå‘˜': 'è½¯ä»¶å·¥ç¨‹å¸ˆ', 
                'Javaå¼€å‘': 'Javaå·¥ç¨‹å¸ˆ',
                'Pythonå¼€å‘': 'Pythonå·¥ç¨‹å¸ˆ',
                'å‰ç«¯å¼€å‘': 'å‰ç«¯å·¥ç¨‹å¸ˆ',
                'åç«¯å¼€å‘': 'åç«¯å·¥ç¨‹å¸ˆ',
            }
            
            item.current_position = position_mapping.get(
                item.current_position, 
                item.current_position
            )
        
        # æŠ€èƒ½æ ‡å‡†åŒ–
        if item.skills:
            normalized_skills = []
            skill_mapping = {
                'javascript': 'JavaScript',
                'python': 'Python',
                'java': 'Java',
                'mysql': 'MySQL',
                'redis': 'Redis',
            }
            
            for skill in item.skills:
                skill_lower = skill.lower().strip()
                normalized_skill = skill_mapping.get(skill_lower, skill.strip())
                if normalized_skill and normalized_skill not in normalized_skills:
                    normalized_skills.append(normalized_skill)
            
            item.skills = normalized_skills
        
        return item

class DatabaseExportPipeline:
    """æ•°æ®åº“å¯¼å‡ºç®¡é“"""
    
    def __init__(self, mysql_settings):
        self.mysql_settings = mysql_settings
        self.connection = None
        
    @classmethod
    def from_crawler(cls, crawler):
        mysql_settings = crawler.settings.getdict("MYSQL_SETTINGS")
        return cls(mysql_settings)
    
    def open_spider(self, spider):
        """çˆ¬è™«å¯åŠ¨æ—¶è¿æ¥æ•°æ®åº“"""
        self.connection = mysql.connector.connect(**self.mysql_settings)
        spider.logger.info("æ•°æ®åº“è¿æ¥å·²å»ºç«‹")
    
    def close_spider(self, spider):
        """çˆ¬è™«ç»“æŸæ—¶å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
            spider.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def process_item(self, item: CandidateItem, spider):
        """å¤„ç†æ•°æ®é¡¹å¹¶æ’å…¥æ•°æ®åº“"""
        try:
            cursor = self.connection.cursor()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if item.email:
                cursor.execute(
                    "SELECT id FROM jobs_candidate WHERE email = %s",
                    (item.email,)
                )
                if cursor.fetchone():
                    spider.logger.info(f"è·³è¿‡é‡å¤é‚®ç®±: {item.email}")
                    return item
            
            # æ’å…¥å€™é€‰äººæ•°æ®
            insert_sql = """
                INSERT INTO jobs_candidate (
                    name, email, phone, current_position, current_company,
                    experience_years, education, skills, resume_file, resume_content,
                    ai_parsed_data, predicted_job_tags, salary_expectation,
                    location_preference, availability, notes, status, source,
                    user_id, created_at, updated_at
                ) VALUES (
                    %(name)s, %(email)s, %(phone)s, %(current_position)s, %(current_company)s,
                    %(experience_years)s, %(education)s, %(skills)s, %(resume_file)s, %(resume_content)s,
                    %(ai_parsed_data)s, %(predicted_job_tags)s, %(salary_expectation)s,
                    %(location_preference)s, %(availability)s, %(notes)s, %(status)s, %(source)s,
                    %(user_id)s, %(created_at)s, %(updated_at)s
                )
            """
            
            data = item.to_dict()
            data['user_id'] = 1  # é»˜è®¤ç”¨æˆ·IDï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
            
            cursor.execute(insert_sql, data)
            self.connection.commit()
            
            spider.logger.info(f"æˆåŠŸæ’å…¥å€™é€‰äºº: {item.name}")
            
        except Exception as e:
            spider.logger.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")
            self.connection.rollback()
        
        finally:
            cursor.close()
        
        return item

class JSONExportPipeline:
    """JSONæ–‡ä»¶å¯¼å‡ºç®¡é“"""
    
    def __init__(self):
        self.file = None
        self.items = []
    
    def open_spider(self, spider):
        """æ‰“å¼€è¾“å‡ºæ–‡ä»¶"""
        filename = f"candidates_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.file = open(filename, 'w', encoding='utf-8')
        spider.logger.info(f"JSONå¯¼å‡ºæ–‡ä»¶: {filename}")
    
    def close_spider(self, spider):
        """å…³é—­æ–‡ä»¶å¹¶å†™å…¥æ•°æ®"""
        if self.file:
            json.dump(self.items, self.file, ensure_ascii=False, indent=2)
            self.file.close()
            spider.logger.info(f"JSONå¯¼å‡ºå®Œæˆï¼Œå…± {len(self.items)} æ¡è®°å½•")
    
    def process_item(self, item: CandidateItem, spider):
        """æ”¶é›†æ•°æ®é¡¹"""
        self.items.append(item.to_dict())
        return item
```

### 3ï¸âƒ£ **æ•°æ®æ ¼å¼åŒ–å™¨ (formatter.py)**

```python
import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime

class CandidateDataFormatter:
    """å€™é€‰äººæ•°æ®æ ¼å¼åŒ–å™¨"""
    
    @staticmethod
    def extract_experience_years(experience_text: str) -> Optional[int]:
        """ä»æ–‡æœ¬ä¸­æå–å·¥ä½œå¹´é™"""
        if not experience_text:
            return None
        
        # åŒ¹é…å„ç§å¹´é™è¡¨è¾¾æ–¹å¼
        patterns = [
            r'(\d+)\s*å¹´',
            r'(\d+)\s*å¹´å·¥ä½œç»éªŒ',
            r'å·¥ä½œ\s*(\d+)\s*å¹´',
            r'(\d+)\s*years?',
            r'experience:\s*(\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, experience_text, re.IGNORECASE)
            if match:
                years = int(match.group(1))
                return years if 0 <= years <= 50 else None
        
        return None
    
    @staticmethod
    def parse_skills(skills_text: str) -> List[str]:
        """è§£ææŠ€èƒ½æ–‡æœ¬ä¸ºæŠ€èƒ½åˆ—è¡¨"""
        if not skills_text:
            return []
        
        # å¸¸è§åˆ†éš”ç¬¦
        separators = [',', 'ï¼Œ', ';', 'ï¼›', '|', '/', 'ã€', '\n', '\t']
        
        # åˆ†å‰²æŠ€èƒ½
        skills = [skills_text]
        for sep in separators:
            new_skills = []
            for skill in skills:
                new_skills.extend(skill.split(sep))
            skills = new_skills
        
        # æ¸…ç†å’Œæ ‡å‡†åŒ–
        cleaned_skills = []
        for skill in skills:
            skill = skill.strip()
            if skill and len(skill) > 1:
                cleaned_skills.append(skill)
        
        return list(set(cleaned_skills))  # å»é‡
    
    @staticmethod
    def format_salary(salary_text: str) -> Optional[str]:
        """æ ¼å¼åŒ–è–ªèµ„ä¿¡æ¯"""
        if not salary_text:
            return None
        
        # æ ‡å‡†åŒ–è–ªèµ„æ ¼å¼
        salary = salary_text.strip()
        
        # å¸¸è§è–ªèµ„æ ¼å¼æ˜ å°„
        salary_patterns = [
            (r'(\d+)[kK][-~](\d+)[kK]', r'\1K-\2K'),
            (r'(\d+)ä¸‡[-~](\d+)ä¸‡', r'\1ä¸‡-\2ä¸‡'),
            (r'å¹´è–ª(\d+)ä¸‡', r'å¹´è–ª\1ä¸‡'),
            (r'æœˆè–ª(\d+)[kK]', r'æœˆè–ª\1K'),
        ]
        
        for pattern, replacement in salary_patterns:
            salary = re.sub(pattern, replacement, salary, flags=re.IGNORECASE)
        
        return salary
    
    @staticmethod
    def generate_ai_parsed_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆAIè§£ææ•°æ®æ ¼å¼"""
        
        ai_data = {
            "extraction_info": {
                "model_used": "crawler_extraction",
                "confidence": 0.8,
                "extraction_time": datetime.now().isoformat(),
                "source_type": "web_crawling"
            },
            "raw_source": {
                "url": raw_data.get('source_url'),
                "platform": raw_data.get('platform'),
                "crawl_time": datetime.now().isoformat()
            }
        }
        
        # æ·»åŠ ä¸ªäººä¿¡æ¯
        if any(key in raw_data for key in ['name', 'email', 'phone']):
            ai_data["personal_info"] = {
                "full_name": raw_data.get('name'),
                "contact_info": {
                    "emails": [raw_data['email']] if raw_data.get('email') else [],
                    "phones": [raw_data['phone']] if raw_data.get('phone') else []
                }
            }
        
        # æ·»åŠ å·¥ä½œç»éªŒ
        if raw_data.get('current_company') or raw_data.get('current_position'):
            ai_data["work_experience"] = [{
                "company": raw_data.get('current_company'),
                "position": raw_data.get('current_position'),
                "is_current": True
            }]
        
        # æ·»åŠ æŠ€èƒ½åˆ†æ
        if raw_data.get('skills'):
            ai_data["skills_analysis"] = {
                "technical_skills": raw_data['skills'],
                "extraction_method": "text_parsing"
            }
        
        return ai_data

class DataExporter:
    """æ•°æ®å¯¼å‡ºå™¨"""
    
    @staticmethod
    def export_to_sql(candidates: List[CandidateItem], filename: str) -> None:
        """å¯¼å‡ºä¸ºSQLæ’å…¥è¯­å¥"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("-- å€™é€‰äººæ•°æ®å¯¼å…¥SQL\n")
            f.write("-- ç”Ÿæˆæ—¶é—´: {}\n\n".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            
            f.write("SET NAMES utf8mb4;\n")
            f.write("SET FOREIGN_KEY_CHECKS = 0;\n\n")
            
            for candidate in candidates:
                data = candidate.to_dict()
                
                # ç”ŸæˆINSERTè¯­å¥
                f.write("INSERT INTO jobs_candidate (\n")
                f.write("    name, email, phone, current_position, current_company,\n")
                f.write("    experience_years, education, skills, resume_content,\n")
                f.write("    ai_parsed_data, predicted_job_tags, salary_expectation,\n")
                f.write("    location_preference, availability, notes, status, source,\n")
                f.write("    user_id, created_at, updated_at\n")
                f.write(") VALUES (\n")
                
                # æ ¼å¼åŒ–å€¼
                values = []
                for key in ['name', 'email', 'phone', 'current_position', 'current_company']:
                    val = data.get(key)
                    values.append(f"'{val}'" if val else 'NULL')
                
                values.append(str(data.get('experience_years')) if data.get('experience_years') else 'NULL')
                
                for key in ['education', 'skills', 'resume_content', 'ai_parsed_data', 
                           'predicted_job_tags', 'salary_expectation', 'location_preference',
                           'availability', 'notes', 'status', 'source']:
                    val = data.get(key)
                    if val:
                        # è½¬ä¹‰å•å¼•å·
                        escaped_val = str(val).replace("'", "\\'")
                        values.append(f"'{escaped_val}'")
                    else:
                        values.append('NULL')
                
                values.append('1')  # user_id
                values.append(f"'{data['created_at'].strftime('%Y-%m-%d %H:%M:%S')}'")
                values.append(f"'{data['updated_at'].strftime('%Y-%m-%d %H:%M:%S')}'")
                
                f.write("    " + ",\n    ".join(values) + "\n")
                f.write(");\n\n")
            
            f.write("SET FOREIGN_KEY_CHECKS = 1;\n")
    
    @staticmethod
    def export_to_api_format(candidates: List[CandidateItem]) -> List[Dict[str, Any]]:
        """å¯¼å‡ºä¸ºAPIæ¥å£æ ¼å¼"""
        
        api_data = []
        for candidate in candidates:
            data = candidate.to_dict()
            
            # APIæ ¼å¼è½¬æ¢
            api_item = {
                "candidate_info": {
                    "basic": {
                        "name": data['name'],
                        "email": data['email'],
                        "phone": data['phone']
                    },
                    "career": {
                        "current_position": data['current_position'],
                        "current_company": data['current_company'],
                        "experience_years": data['experience_years'],
                        "education": data['education']
                    },
                    "skills": json.loads(data['skills']) if data['skills'] else [],
                    "preferences": {
                        "salary_expectation": data['salary_expectation'],
                        "location_preference": data['location_preference'],
                        "availability": data['availability']
                    }
                },
                "metadata": {
                    "source": data['source'],
                    "status": data['status'],
                    "notes": data['notes'],
                    "created_at": data['created_at'].isoformat(),
                    "updated_at": data['updated_at'].isoformat()
                },
                "ai_data": json.loads(data['ai_parsed_data']) if data['ai_parsed_data'] else {}
            }
            
            api_data.append(api_item)
        
        return api_data
```

---

## ğŸ”Œ **APIæ¥å£è®¾è®¡**

### 1ï¸âƒ£ **RESTful APIç«¯ç‚¹**

```python
# api/views.py
from rest_framework import viewsets, status
from rest_framework.decorators import action
from rest_framework.response import Response
from .models import CandidateData
from .serializers import CandidateSerializer

class CandidateDataViewSet(viewsets.ModelViewSet):
    """å€™é€‰äººæ•°æ®APIè§†å›¾é›†"""
    
    queryset = CandidateData.objects.all()
    serializer_class = CandidateSerializer
    
    @action(detail=False, methods=['post'])
    def batch_import(self, request):
        """æ‰¹é‡å¯¼å…¥å€™é€‰äººæ•°æ®"""
        data = request.data.get('candidates', [])
        
        results = {
            'total': len(data),
            'success': 0,
            'failed': 0,
            'errors': []
        }
        
        for item in data:
            try:
                serializer = self.get_serializer(data=item)
                if serializer.is_valid():
                    serializer.save()
                    results['success'] += 1
                else:
                    results['failed'] += 1
                    results['errors'].append({
                        'data': item,
                        'errors': serializer.errors
                    })
            except Exception as e:
                results['failed'] += 1
                results['errors'].append({
                    'data': item,
                    'error': str(e)
                })
        
        return Response(results, status=status.HTTP_200_OK)
    
    @action(detail=False, methods=['get'])
    def export_format(self, request):
        """è·å–æ•°æ®å¯¼å‡ºæ ¼å¼è¯´æ˜"""
        format_info = {
            'required_fields': ['name'],
            'optional_fields': [
                'email', 'phone', 'current_position', 'current_company',
                'experience_years', 'education', 'skills', 'salary_expectation',
                'location_preference', 'availability', 'notes', 'source'
            ],
            'data_formats': {
                'skills': 'JSONæ•°ç»„æˆ–é€—å·åˆ†éš”å­—ç¬¦ä¸²',
                'experience_years': 'æ•´æ•°',
                'salary_expectation': 'å­—ç¬¦ä¸²æè¿°',
                'email': 'æœ‰æ•ˆé‚®ç®±æ ¼å¼',
                'phone': '11ä½æ‰‹æœºå·'
            },
            'example': {
                'name': 'å¼ ä¸‰',
                'email': 'zhangsan@example.com',
                'phone': '13800138000',
                'current_position': 'Pythonå·¥ç¨‹å¸ˆ',
                'current_company': 'å­—èŠ‚è·³åŠ¨',
                'experience_years': 5,
                'skills': ['Python', 'Django', 'MySQL'],
                'salary_expectation': 'å¹´è–ª50-60ä¸‡',
                'location_preference': 'åŒ—äº¬',
                'availability': 'éšæ—¶åˆ°å²—',
                'source': 'ç½‘ç»œçˆ¬è™«'
            }
        }
        
        return Response(format_info)
```

### 2ï¸âƒ£ **æ•°æ®åºåˆ—åŒ–å™¨**

```python
# api/serializers.py
from rest_framework import serializers
from .models import CandidateData
import json

class CandidateSerializer(serializers.ModelSerializer):
    """å€™é€‰äººæ•°æ®åºåˆ—åŒ–å™¨"""
    
    skills_list = serializers.ListField(
        child=serializers.CharField(),
        write_only=True,
        required=False,
        help_text="æŠ€èƒ½åˆ—è¡¨"
    )
    
    class Meta:
        model = CandidateData
        fields = '__all__'
        extra_kwargs = {
            'name': {'required': True, 'help_text': 'å€™é€‰äººå§“å'},
            'email': {'required': False, 'help_text': 'é‚®ç®±åœ°å€'},
            'phone': {'required': False, 'help_text': 'æ‰‹æœºå·ç '},
            'skills': {'read_only': True},
        }
    
    def validate_email(self, value):
        """éªŒè¯é‚®ç®±æ ¼å¼"""
        if value:
            import re
            pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            if not re.match(pattern, value):
                raise serializers.ValidationError("é‚®ç®±æ ¼å¼ä¸æ­£ç¡®")
        return value
    
    def validate_phone(self, value):
        """éªŒè¯æ‰‹æœºå·æ ¼å¼"""
        if value:
            import re
            phone_clean = re.sub(r'[^\d]', '', value)
            if not (len(phone_clean) == 11 and phone_clean.startswith('1')):
                raise serializers.ValidationError("æ‰‹æœºå·æ ¼å¼ä¸æ­£ç¡®")
            return phone_clean
        return value
    
    def validate_experience_years(self, value):
        """éªŒè¯å·¥ä½œå¹´é™"""
        if value is not None:
            if not isinstance(value, int) or value < 0 or value > 50:
                raise serializers.ValidationError("å·¥ä½œå¹´é™å¿…é¡»æ˜¯0-50ä¹‹é—´çš„æ•´æ•°")
        return value
    
    def create(self, validated_data):
        """åˆ›å»ºå€™é€‰äººè®°å½•"""
        skills_list = validated_data.pop('skills_list', [])
        
        # å¤„ç†æŠ€èƒ½åˆ—è¡¨
        if skills_list:
            validated_data['skills'] = json.dumps(skills_list, ensure_ascii=False)
        
        return super().create(validated_data)
    
    def to_representation(self, instance):
        """è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼"""
        data = super().to_representation(instance)
        
        # è§£ææŠ€èƒ½JSON
        if data.get('skills'):
            try:
                data['skills_list'] = json.loads(data['skills'])
            except json.JSONDecodeError:
                data['skills_list'] = []
        
        # è§£æAIæ•°æ®
        if data.get('ai_parsed_data'):
            try:
                data['ai_parsed_data'] = json.loads(data['ai_parsed_data'])
            except json.JSONDecodeError:
                data['ai_parsed_data'] = {}
        
        return data
```

---

## ğŸ“š **ä½¿ç”¨æ–‡æ¡£**

### 1ï¸âƒ£ **å¿«é€Ÿå¼€å§‹**

```bash
# 1. åˆ›å»ºé¡¹ç›®
mkdir candidate_crawler
cd candidate_crawler

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 3. å®‰è£…ä¾èµ–
pip install scrapy django djangorestframework mysql-connector-python

# 4. åˆ›å»ºDjangoé¡¹ç›®
django-admin startproject api_server
cd api_server
python manage.py startapp candidates

# 5. åˆ›å»ºScrapyé¡¹ç›®
cd ..
scrapy startproject crawler
```

### 2ï¸âƒ£ **é…ç½®ç¤ºä¾‹**

```python
# settings.py (Scrapy)
ITEM_PIPELINES = {
    'crawler.pipelines.DataValidationPipeline': 100,
    'crawler.pipelines.DataNormalizationPipeline': 200,
    'crawler.pipelines.JSONExportPipeline': 300,
    'crawler.pipelines.DatabaseExportPipeline': 400,
}

MYSQL_SETTINGS = {
    'host': 'localhost',
    'port': 3306,
    'user': 'headhunter_user',
    'password': 'your_password',
    'database': 'headhunter_db',
    'charset': 'utf8mb4'
}

# Django settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'headhunter_db',
        'USER': 'headhunter_user',
        'PASSWORD': 'your_password',
        'HOST': 'localhost',
        'PORT': '3306',
    }
}

REST_FRAMEWORK = {
    'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination',
    'PAGE_SIZE': 50
}
```

### 3ï¸âƒ£ **APIè°ƒç”¨ç¤ºä¾‹**

```python
import requests
import json

# æ‰¹é‡å¯¼å…¥å€™é€‰äººæ•°æ®
candidates_data = [
    {
        "name": "å¼ ä¸‰",
        "email": "zhangsan@example.com",
        "phone": "13800138000",
        "current_position": "Pythonå·¥ç¨‹å¸ˆ",
        "current_company": "å­—èŠ‚è·³åŠ¨",
        "experience_years": 5,
        "skills_list": ["Python", "Django", "MySQL", "Redis"],
        "salary_expectation": "å¹´è–ª50-60ä¸‡",
        "location_preference": "åŒ—äº¬",
        "source": "ç½‘ç»œçˆ¬è™«"
    }
]

response = requests.post(
    'http://localhost:8000/api/candidates/batch_import/',
    json={'candidates': candidates_data},
    headers={'Content-Type': 'application/json'}
)

print(response.json())
```

---

## ğŸš€ **é¡¹ç›®æ¨èå·¥å…·å’Œæ¡†æ¶**

### ğŸ“¦ **ä¾èµ–åŒ…æ¨è**

```txt
# requirements.txt
# çˆ¬è™«æ¡†æ¶
scrapy==2.11.0
scrapy-splash==0.8.0

# Webæ¡†æ¶  
django==4.2.7
djangorestframework==3.14.0

# æ•°æ®åº“
mysql-connector-python==8.2.0
PyMySQL==1.1.0

# æ•°æ®å¤„ç†
pandas==2.1.3
numpy==1.24.3

# æ–‡æœ¬å¤„ç†
jieba==0.42.1
python-docx==1.1.0
PyPDF2==3.0.1

# å·¥å…·åº“
requests==2.31.0
lxml==4.9.3
beautifulsoup4==4.12.2
fake-useragent==1.4.0

# å¼‚æ­¥æ”¯æŒ
aiohttp==3.9.1
asyncio==3.4.3

# æ—¥å¿—å’Œç›‘æ§
loguru==0.7.2
```

---

## ğŸ“Š **å­—æ®µæ˜ å°„å¯¹ç…§è¡¨**

| ç›®æ ‡å­—æ®µ | æ•°æ®ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|---------|---------|------|------|
| `name` | varchar(100) | å€™é€‰äººå§“åï¼Œå¿…å¡« | "å¼ ä¸‰" |
| `email` | varchar(254) | é‚®ç®±åœ°å€ï¼Œç”¨äºå»é‡ | "zhangsan@example.com" |
| `phone` | varchar(20) | æ‰‹æœºå·ç ï¼Œ11ä½æ•°å­— | "13800138000" |
| `current_position` | varchar(200) | å½“å‰èŒä½ | "é«˜çº§Pythonå·¥ç¨‹å¸ˆ" |
| `current_company` | varchar(200) | å½“å‰å…¬å¸ | "å­—èŠ‚è·³åŠ¨" |
| `experience_years` | int(11) | å·¥ä½œå¹´é™ï¼Œæ•´æ•° | 5 |
| `education` | varchar(200) | æœ€é«˜å­¦å† | "æœ¬ç§‘" |
| `skills` | longtext | æŠ€èƒ½æ ‡ç­¾ï¼ŒJSONæ•°ç»„æ ¼å¼ | '["Python", "Django"]' |
| `predicted_job_tags` | longtext | AIé¢„æµ‹èŒä½æ ‡ç­¾ï¼ŒJSON | '["åç«¯å·¥ç¨‹å¸ˆ", "å…¨æ ˆå·¥ç¨‹å¸ˆ"]' |
| `resume_content` | longtext | ç®€å†æ–‡æœ¬å†…å®¹ | "ä¸ªäººç®€å†å†…å®¹..." |
| `ai_parsed_data` | longtext | AIè§£æç»“æ„åŒ–æ•°æ®ï¼ŒJSON | '{"personal_info": {...}}' |
| `salary_expectation` | varchar(100) | æœŸæœ›è–ªèµ„ | "å¹´è–ª50-60ä¸‡" |
| `location_preference` | varchar(200) | æœŸæœ›å·¥ä½œåœ°ç‚¹ | "åŒ—äº¬" |
| `availability` | varchar(50) | å¯å…¥èŒæ—¶é—´ | "éšæ—¶åˆ°å²—" |
| `notes` | longtext | å¤‡æ³¨ä¿¡æ¯ | "ä»XXç½‘ç«™çˆ¬å–" |
| `status` | varchar(20) | çŠ¶æ€ | "active" |
| `source` | varchar(100) | æ•°æ®æ¥æºæ¸ é“ | "ç½‘ç»œçˆ¬è™«" |
| `user_id` | bigint(20) | æ‰€å±ç”¨æˆ·ID | 1 |
| `created_at` | datetime(6) | åˆ›å»ºæ—¶é—´ | "2024-01-15 10:30:00" |
| `updated_at` | datetime(6) | æ›´æ–°æ—¶é—´ | "2024-01-15 10:30:00" |

---

## ğŸ¯ **é¡¹ç›®å®æ–½å»ºè®®**

### 1ï¸âƒ£ **å¼€å‘é˜¶æ®µè§„åˆ’**

1. **é˜¶æ®µ1: åŸºç¡€æ¡†æ¶æ­å»º**
   - åˆ›å»ºScrapyå’ŒDjangoé¡¹ç›®
   - é…ç½®æ•°æ®åº“è¿æ¥
   - å®ç°åŸºç¡€æ•°æ®æ¨¡å‹

2. **é˜¶æ®µ2: çˆ¬è™«å¼€å‘**
   - ç¼–å†™ç›®æ ‡ç½‘ç«™çˆ¬è™«
   - å®ç°æ•°æ®æå–é€»è¾‘
   - æ·»åŠ æ•°æ®éªŒè¯å’Œæ¸…æ´—

3. **é˜¶æ®µ3: APIæ¥å£å¼€å‘**
   - å®ç°RESTful API
   - æ·»åŠ æ‰¹é‡å¯¼å…¥åŠŸèƒ½
   - å®Œå–„æ•°æ®åºåˆ—åŒ–

4. **é˜¶æ®µ4: æ•°æ®å¤„ç†ä¼˜åŒ–**
   - ä¼˜åŒ–æ•°æ®æ ¼å¼åŒ–é€»è¾‘
   - æ·»åŠ é”™è¯¯å¤„ç†æœºåˆ¶
   - å®ç°æ•°æ®å¯¼å‡ºåŠŸèƒ½

### 2ï¸âƒ£ **æŠ€æœ¯é€‰å‹å»ºè®®**

- **çˆ¬è™«æ¡†æ¶**: Scrapy (åŠŸèƒ½å®Œå–„ï¼Œæ‰©å±•æ€§å¥½)
- **Webæ¡†æ¶**: Django + DRF (å¿«é€Ÿå¼€å‘ï¼ŒAPIå‹å¥½)
- **æ•°æ®åº“**: MySQL (ä¸ç›®æ ‡ç³»ç»Ÿå…¼å®¹)
- **æ•°æ®å¤„ç†**: Pandas (æ•°æ®åˆ†æå’Œæ¸…æ´—)
- **ä»»åŠ¡é˜Ÿåˆ—**: Celery (å¯é€‰ï¼Œå¤„ç†å¤§æ‰¹é‡æ•°æ®)

### 3ï¸âƒ£ **éƒ¨ç½²å»ºè®®**

- **å¼€å‘ç¯å¢ƒ**: ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒéš”ç¦»ä¾èµ–
- **æµ‹è¯•ç¯å¢ƒ**: å°æ‰¹é‡æ•°æ®æµ‹è¯•
- **ç”Ÿäº§ç¯å¢ƒ**: Dockerå®¹å™¨åŒ–éƒ¨ç½²
- **ç›‘æ§**: æ·»åŠ æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§

è¿™ä¸ªæ–‡æ¡£æä¾›äº†å®Œæ•´çš„æ•°æ®ç»“æ„ã€ä»£ç æ¡†æ¶å’Œå®æ–½æŒ‡å—ï¼Œå¯ä»¥ä½œä¸ºçˆ¬è™«é¡¹ç›®å¼€å‘çš„æŠ€æœ¯å‚è€ƒæ–‡æ¡£ï¼ ğŸš€