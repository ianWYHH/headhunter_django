# üìã Êï∞ÊçÆÁªìÊûÑ‰∏éAPIËÆæËÆ°ÊñáÊ°£ - ÂÄôÈÄâ‰∫∫Êï∞ÊçÆÂ§ÑÁêÜÈ°πÁõÆ

## üèóÔ∏è **È°πÁõÆÊû∂ÊûÑÂª∫ËÆÆ**

```
crawler_project/                    # Áà¨Ëô´Êï∞ÊçÆÂ§ÑÁêÜÈ°πÁõÆ
‚îú‚îÄ‚îÄ üìÅ crawler/                     # Áà¨Ëô´Ê®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ spiders/                    # Áà¨Ëô´ËÑöÊú¨
‚îÇ   ‚îú‚îÄ‚îÄ items.py                    # Êï∞ÊçÆÈ°πÂÆö‰πâ
‚îÇ   ‚îú‚îÄ‚îÄ pipelines.py                # Êï∞ÊçÆÂ§ÑÁêÜÁÆ°ÈÅì
‚îÇ   ‚îî‚îÄ‚îÄ settings.py                 # Áà¨Ëô´ÈÖçÁΩÆ
‚îú‚îÄ‚îÄ üìÅ processor/                   # Êï∞ÊçÆÂ§ÑÁêÜÊ®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py               # Êï∞ÊçÆÊ†áÂáÜÂåñ
‚îÇ   ‚îú‚îÄ‚îÄ validator.py                # Êï∞ÊçÆÈ™åËØÅ
‚îÇ   ‚îî‚îÄ‚îÄ formatter.py                # Êï∞ÊçÆÊ†ºÂºèÂåñ
‚îú‚îÄ‚îÄ üìÅ api/                         # APIÊé•Âè£Ê®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ models.py                   # Êï∞ÊçÆÊ®°Âûã
‚îÇ   ‚îú‚îÄ‚îÄ views.py                    # APIËßÜÂõæ
‚îÇ   ‚îî‚îÄ‚îÄ serializers.py              # Êï∞ÊçÆÂ∫èÂàóÂåñ
‚îú‚îÄ‚îÄ üìÅ output/                      # ËæìÂá∫Ê®°Âùó
‚îÇ   ‚îú‚îÄ‚îÄ json_exporter.py            # JSONÂØºÂá∫
‚îÇ   ‚îú‚îÄ‚îÄ sql_generator.py            # SQLÁîüÊàê
‚îÇ   ‚îî‚îÄ‚îÄ api_sender.py               # APIÂèëÈÄÅ
‚îî‚îÄ‚îÄ üìÑ requirements.txt             # È°πÁõÆ‰æùËµñ
```

---

## üóÑÔ∏è **ÁõÆÊ†áÁ≥ªÁªüÊï∞ÊçÆÁªìÊûÑ**

### 1Ô∏è‚É£ **ÂÄôÈÄâ‰∫∫Ê†∏ÂøÉÊï∞ÊçÆÁªìÊûÑ**

#### üìä **Êï∞ÊçÆÂ∫ìË°®ÁªìÊûÑ**

```sql
-- ===============================================
-- ÂÄôÈÄâ‰∫∫‰ø°ÊÅØË°® (jobs_candidate)
-- ===============================================
CREATE TABLE `jobs_candidate` (
  -- Âü∫Á°ÄÂ≠óÊÆµ
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '‰∏ªÈîÆID',
  `name` varchar(100) NOT NULL COMMENT 'ÂÄôÈÄâ‰∫∫ÂßìÂêç',
  `email` varchar(254) DEFAULT NULL COMMENT 'ÈÇÆÁÆ±Âú∞ÂùÄ',
  `phone` varchar(20) DEFAULT NULL COMMENT 'ÊâãÊú∫Âè∑Á†Å',
  
  -- ËÅå‰∏ö‰ø°ÊÅØ
  `current_position` varchar(200) DEFAULT NULL COMMENT 'ÂΩìÂâçËÅå‰Ωç',
  `current_company` varchar(200) DEFAULT NULL COMMENT 'ÂΩìÂâçÂÖ¨Âè∏',
  `experience_years` int(11) DEFAULT NULL COMMENT 'Â∑•‰ΩúÂπ¥Èôê(Êï¥Êï∞)',
  `education` varchar(200) DEFAULT NULL COMMENT 'ÊúÄÈ´òÂ≠¶ÂéÜ',
  
  -- ÊäÄËÉΩÂíåËÉΩÂäõ
  `skills` longtext DEFAULT NULL COMMENT 'ÊäÄËÉΩÊ†áÁ≠æ(JSONÊï∞ÁªÑÊ†ºÂºè)',
  `predicted_job_tags` longtext DEFAULT NULL COMMENT 'AIÈ¢ÑÊµãÈÄÇÂêàËÅå‰ΩçÊ†áÁ≠æ(JSONÊï∞ÁªÑ)',
  
  -- ÁÆÄÂéÜÁõ∏ÂÖ≥
  `resume_file` varchar(100) DEFAULT NULL COMMENT 'ÁÆÄÂéÜÊñá‰ª∂Ë∑ØÂæÑ',
  `resume_content` longtext DEFAULT NULL COMMENT 'ÁÆÄÂéÜÊñáÊú¨ÂÜÖÂÆπ',
  `ai_parsed_data` longtext DEFAULT NULL COMMENT 'AIËß£ÊûêÁöÑÁªìÊûÑÂåñÊï∞ÊçÆ(JSON)',
  
  -- Ê±ÇËÅåÊÑèÂêë
  `salary_expectation` varchar(100) DEFAULT NULL COMMENT 'ÊúüÊúõËñ™ËµÑ',
  `location_preference` varchar(200) DEFAULT NULL COMMENT 'ÊúüÊúõÂ∑•‰ΩúÂú∞ÁÇπ',
  `availability` varchar(50) DEFAULT NULL COMMENT 'ÂèØÂÖ•ËÅåÊó∂Èó¥',
  
  -- ÁÆ°ÁêÜÂ≠óÊÆµ
  `notes` longtext DEFAULT NULL COMMENT 'Â§áÊ≥®‰ø°ÊÅØ',
  `status` varchar(20) DEFAULT 'active' COMMENT 'Áä∂ÊÄÅ(active/inactive/hired/blacklist)',
  `source` varchar(100) DEFAULT NULL COMMENT 'Êï∞ÊçÆÊù•Ê∫êÊ∏†ÈÅì',
  `user_id` bigint(20) NOT NULL COMMENT 'ÊâÄÂ±ûÁî®Êà∑ID',
  
  -- Êó∂Èó¥Êà≥
  `created_at` datetime(6) NOT NULL COMMENT 'ÂàõÂª∫Êó∂Èó¥',
  `updated_at` datetime(6) NOT NULL COMMENT 'Êõ¥Êñ∞Êó∂Èó¥',
  
  -- Á¥¢Âºï
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_email_user` (`email`, `user_id`),
  KEY `idx_phone` (`phone`),
  KEY `idx_status` (`status`),
  KEY `idx_created_at` (`created_at`),
  KEY `idx_user_id` (`user_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='ÂÄôÈÄâ‰∫∫‰ø°ÊÅØË°®';

-- ===============================================
-- ÂÄôÈÄâ‰∫∫ÂàÜÁªÑË°® (jobs_candidategroup)
-- ===============================================
CREATE TABLE `jobs_candidategroup` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'ÂàÜÁªÑID',
  `name` varchar(100) NOT NULL COMMENT 'ÂàÜÁªÑÂêçÁß∞',
  `description` longtext DEFAULT NULL COMMENT 'ÂàÜÁªÑÊèèËø∞',
  `color` varchar(20) DEFAULT '#007bff' COMMENT 'ÂàÜÁªÑÈ¢úËâ≤‰ª£Á†Å',
  `tags` longtext DEFAULT NULL COMMENT 'ÂàÜÁªÑÊ†áÁ≠æ(JSONÊï∞ÁªÑ)',
  `user_id` bigint(20) NOT NULL COMMENT 'ÊâÄÂ±ûÁî®Êà∑ID',
  `created_at` datetime(6) NOT NULL COMMENT 'ÂàõÂª∫Êó∂Èó¥',
  `updated_at` datetime(6) NOT NULL COMMENT 'Êõ¥Êñ∞Êó∂Èó¥',
  
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_name_user` (`name`, `user_id`),
  KEY `idx_user_id` (`user_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='ÂÄôÈÄâ‰∫∫ÂàÜÁªÑË°®';

-- ===============================================
-- ÂÄôÈÄâ‰∫∫ÂàÜÁªÑÂÖ≥ËÅîË°® (jobs_candidate_groups)
-- ===============================================
CREATE TABLE `jobs_candidate_groups` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `candidate_id` bigint(20) NOT NULL COMMENT 'ÂÄôÈÄâ‰∫∫ID',
  `candidategroup_id` bigint(20) NOT NULL COMMENT 'ÂàÜÁªÑID',
  
  PRIMARY KEY (`id`),
  UNIQUE KEY `unique_candidate_group` (`candidate_id`, `candidategroup_id`),
  KEY `idx_candidate_id` (`candidate_id`),
  KEY `idx_group_id` (`candidategroup_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='ÂÄôÈÄâ‰∫∫ÂàÜÁªÑÂÖ≥ËÅîË°®';
```

### 2Ô∏è‚É£ **JSONÊï∞ÊçÆÊ†ºÂºèËßÑËåÉ**

#### üè∑Ô∏è **ÊäÄËÉΩÊ†áÁ≠æÊ†ºÂºè (skills)**
```json
{
  "programming_languages": ["Python", "Java", "JavaScript", "Go"],
  "frameworks": ["Django", "Spring Boot", "React", "Vue.js"],
  "databases": ["MySQL", "Redis", "MongoDB", "PostgreSQL"],
  "tools": ["Git", "Docker", "Kubernetes", "Jenkins"],
  "soft_skills": ["Âõ¢ÈòüÂçè‰Ωú", "È°πÁõÆÁÆ°ÁêÜ", "Ê≤üÈÄöËÉΩÂäõ"],
  "certifications": ["AWSËÆ§ËØÅ", "PMPËÆ§ËØÅ"],
  "languages": [
    {"language": "‰∏≠Êñá", "level": "ÊØçËØ≠"},
    {"language": "Ëã±ËØ≠", "level": "ÁÜüÁªÉ"}
  ]
}
```

#### üéØ **AIÈ¢ÑÊµãËÅå‰ΩçÊ†áÁ≠æÊ†ºÂºè (predicted_job_tags)**
```json
{
  "primary_roles": [
    {"role": "ÂêéÁ´ØÂºÄÂèëÂ∑•Á®ãÂ∏à", "confidence": 0.95},
    {"role": "ÂÖ®Ê†àÂºÄÂèëÂ∑•Á®ãÂ∏à", "confidence": 0.85}
  ],
  "secondary_roles": [
    {"role": "ÊäÄÊúØÁªèÁêÜ", "confidence": 0.70},
    {"role": "Êû∂ÊûÑÂ∏à", "confidence": 0.65}
  ],
  "industries": ["‰∫íËÅîÁΩë", "ÈáëËûçÁßëÊäÄ", "ÁîµÂïÜ"],
  "seniority_level": "‰∏≠Á∫ß", // ÂàùÁ∫ß/‰∏≠Á∫ß/È´òÁ∫ß/‰∏ìÂÆ∂
  "job_categories": ["ÊäÄÊúØ", "Á†îÂèë", "ÂêéÁ´Ø"]
}
```

#### ü§ñ **AIËß£ÊûêÊï∞ÊçÆÊ†ºÂºè (ai_parsed_data)**
```json
{
  "extraction_info": {
    "model_used": "qwen-plus",
    "confidence": 0.92,
    "extraction_time": "2024-01-15T10:30:00Z",
    "source_type": "resume_pdf"
  },
  "personal_info": {
    "full_name": "Âº†‰∏â",
    "gender": "Áî∑",
    "age": 28,
    "birth_year": 1995,
    "marital_status": "Êú™Â©ö"
  },
  "contact_info": {
    "emails": ["zhangsan@example.com"],
    "phones": ["13800138000"],
    "location": "Âåó‰∫¨Â∏ÇÊúùÈò≥Âå∫",
    "address": "ÂÖ∑‰ΩìÂú∞ÂùÄ‰ø°ÊÅØ"
  },
  "education": [
    {
      "school": "Ê∏ÖÂçéÂ§ßÂ≠¶",
      "degree": "Êú¨Áßë",
      "major": "ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏éÊäÄÊúØ",
      "start_date": "2013-09",
      "end_date": "2017-07",
      "gpa": "3.8/4.0"
    }
  ],
  "work_experience": [
    {
      "company": "Â≠óËäÇË∑≥Âä®",
      "position": "È´òÁ∫ßÂêéÁ´ØÂºÄÂèëÂ∑•Á®ãÂ∏à",
      "start_date": "2020-03",
      "end_date": "2024-01",
      "duration": "3Âπ¥10‰∏™Êúà",
      "responsibilities": [
        "Ë¥üË¥£Êé®ËçêÁ≥ªÁªüÂêéÁ´ØÂºÄÂèë",
        "‰ºòÂåñÁ≥ªÁªüÊÄßËÉΩÔºåÊèêÂçáQPS 50%",
        "Â∏¶È¢Ü3‰∫∫Â∞èÂõ¢ÈòüÂÆåÊàêÈ°πÁõÆ"
      ],
      "technologies": ["Python", "Django", "MySQL", "Redis"]
    }
  ],
  "projects": [
    {
      "name": "Áî®Êà∑Êé®ËçêÁ≥ªÁªü",
      "role": "Ê†∏ÂøÉÂºÄÂèë",
      "duration": "6‰∏™Êúà",
      "description": "ÊûÑÂª∫ÂçÉ‰∏áÁ∫ßÁî®Êà∑Êé®ËçêÁ≥ªÁªü",
      "technologies": ["Python", "TensorFlow", "Redis"],
      "achievements": ["ÊèêÂçáÁÇπÂáªÁéá15%", "Êó•Â§ÑÁêÜËØ∑Ê±Ç1‰∫øÊ¨°"]
    }
  ],
  "skills_analysis": {
    "technical_skills": {
      "programming": ["Python", "Java", "JavaScript"],
      "frameworks": ["Django", "Spring"],
      "databases": ["MySQL", "Redis"],
      "proficiency_level": "È´òÁ∫ß"
    },
    "years_experience": {
      "total": 6,
      "python": 5,
      "backend": 6,
      "team_lead": 2
    }
  },
  "salary_info": {
    "current_salary": "Âπ¥Ëñ™40‰∏á",
    "expected_salary": "Âπ¥Ëñ™50-60‰∏á",
    "currency": "CNY"
  }
}
```

---

## üìù **Êï∞ÊçÆÂ§ÑÁêÜÈ°πÁõÆ‰ª£Á†ÅÁªìÊûÑ**

### 1Ô∏è‚É£ **Êï∞ÊçÆÈ°πÂÆö‰πâ (items.py)**

```python
import scrapy
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from datetime import datetime

@dataclass
class CandidateItem:
    """ÂÄôÈÄâ‰∫∫Êï∞ÊçÆÈ°π"""
    
    # Âü∫Á°Ä‰ø°ÊÅØ
    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    
    # ËÅå‰∏ö‰ø°ÊÅØ  
    current_position: Optional[str] = None
    current_company: Optional[str] = None
    experience_years: Optional[int] = None
    education: Optional[str] = None
    
    # ÊäÄËÉΩÂíåÊ†áÁ≠æ
    skills: List[str] = None
    predicted_job_tags: List[str] = None
    
    # ÁÆÄÂéÜÁõ∏ÂÖ≥
    resume_file: Optional[str] = None
    resume_content: Optional[str] = None
    ai_parsed_data: Optional[Dict[str, Any]] = None
    
    # Ê±ÇËÅåÊÑèÂêë
    salary_expectation: Optional[str] = None
    location_preference: Optional[str] = None
    availability: Optional[str] = None
    
    # ÁÆ°ÁêÜÂ≠óÊÆµ
    notes: Optional[str] = None
    status: str = 'active'
    source: Optional[str] = None
    
    # Êó∂Èó¥Êà≥
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    
    # ÂéüÂßãÊï∞ÊçÆ
    raw_data: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏Ê†ºÂºè"""
        return {
            'name': self.name,
            'email': self.email,
            'phone': self.phone,
            'current_position': self.current_position,
            'current_company': self.current_company,
            'experience_years': self.experience_years,
            'education': self.education,
            'skills': self.format_skills(),
            'predicted_job_tags': self.format_job_tags(),
            'resume_file': self.resume_file,
            'resume_content': self.resume_content,
            'ai_parsed_data': self.format_ai_data(),
            'salary_expectation': self.salary_expectation,
            'location_preference': self.location_preference,
            'availability': self.availability,
            'notes': self.notes,
            'status': self.status,
            'source': self.source,
            'created_at': self.created_at or datetime.now(),
            'updated_at': datetime.now()
        }
    
    def format_skills(self) -> str:
        """Ê†ºÂºèÂåñÊäÄËÉΩ‰∏∫JSONÂ≠óÁ¨¶‰∏≤"""
        if not self.skills:
            return '[]'
        import json
        return json.dumps(self.skills, ensure_ascii=False)
    
    def format_job_tags(self) -> str:
        """Ê†ºÂºèÂåñËÅå‰ΩçÊ†áÁ≠æ‰∏∫JSONÂ≠óÁ¨¶‰∏≤"""
        if not self.predicted_job_tags:
            return '[]'
        import json
        return json.dumps(self.predicted_job_tags, ensure_ascii=False)
    
    def format_ai_data(self) -> str:
        """Ê†ºÂºèÂåñAIÊï∞ÊçÆ‰∏∫JSONÂ≠óÁ¨¶‰∏≤"""
        if not self.ai_parsed_data:
            return '{}'
        import json
        return json.dumps(self.ai_parsed_data, ensure_ascii=False)
```

### 2Ô∏è‚É£ **Êï∞ÊçÆÂ§ÑÁêÜÁÆ°ÈÅì (pipelines.py)**

```python
import json
import mysql.connector
from datetime import datetime
import logging
from typing import Dict, Any

class DataValidationPipeline:
    """Êï∞ÊçÆÈ™åËØÅÁÆ°ÈÅì"""
    
    def process_item(self, item: CandidateItem, spider):
        """È™åËØÅÊï∞ÊçÆÈ°π"""
        
        # ÂøÖÂ°´Â≠óÊÆµÈ™åËØÅ
        if not item.name or not item.name.strip():
            raise ValueError("ÂÄôÈÄâ‰∫∫ÂßìÂêç‰∏çËÉΩ‰∏∫Á©∫")
        
        # ÈÇÆÁÆ±Ê†ºÂºèÈ™åËØÅ
        if item.email:
            import re
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            if not re.match(email_pattern, item.email):
                spider.logger.warning(f"ÈÇÆÁÆ±Ê†ºÂºè‰∏çÊ≠£Á°Æ: {item.email}")
                item.email = None
        
        # ÊâãÊú∫Âè∑Ê†ºÂºèÈ™åËØÅ
        if item.phone:
            phone_clean = re.sub(r'[^\d]', '', item.phone)
            if len(phone_clean) == 11 and phone_clean.startswith('1'):
                item.phone = phone_clean
            else:
                spider.logger.warning(f"ÊâãÊú∫Âè∑Ê†ºÂºè‰∏çÊ≠£Á°Æ: {item.phone}")
                item.phone = None
        
        # Â∑•‰ΩúÂπ¥ÈôêÈ™åËØÅ
        if item.experience_years is not None:
            if not isinstance(item.experience_years, int) or item.experience_years < 0 or item.experience_years > 50:
                spider.logger.warning(f"Â∑•‰ΩúÂπ¥Èôê‰∏çÂêàÁêÜ: {item.experience_years}")
                item.experience_years = None
        
        return item

class DataNormalizationPipeline:
    """Êï∞ÊçÆÊ†áÂáÜÂåñÁÆ°ÈÅì"""
    
    def process_item(self, item: CandidateItem, spider):
        """Ê†áÂáÜÂåñÊï∞ÊçÆ"""
        
        # ÂßìÂêçÊ†áÂáÜÂåñ
        if item.name:
            item.name = item.name.strip()
        
        # ÂÖ¨Âè∏ÂêçÁß∞Ê†áÂáÜÂåñ
        if item.current_company:
            # ÁßªÈô§Â∏∏ËßÅÁöÑÂÖ¨Âè∏ÂêéÁºÄÂèò‰Ωì
            company_suffixes = ['ÊúâÈôêÂÖ¨Âè∏', 'ËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏', 'ÁßëÊäÄÊúâÈôêÂÖ¨Âè∏', 'Ltd.', 'Inc.', 'Corp.']
            for suffix in company_suffixes:
                if item.current_company.endswith(suffix):
                    item.current_company = item.current_company[:-len(suffix)].strip()
                    break
        
        # ËÅå‰ΩçÂêçÁß∞Ê†áÂáÜÂåñ
        if item.current_position:
            # Ê†áÂáÜÂåñÂ∏∏ËßÅËÅå‰ΩçÂêçÁß∞
            position_mapping = {
                'ËΩØ‰ª∂ÂºÄÂèëÂ∑•Á®ãÂ∏à': 'ËΩØ‰ª∂Â∑•Á®ãÂ∏à',
                'Á®ãÂ∫èÂëò': 'ËΩØ‰ª∂Â∑•Á®ãÂ∏à', 
                'JavaÂºÄÂèë': 'JavaÂ∑•Á®ãÂ∏à',
                'PythonÂºÄÂèë': 'PythonÂ∑•Á®ãÂ∏à',
                'ÂâçÁ´ØÂºÄÂèë': 'ÂâçÁ´ØÂ∑•Á®ãÂ∏à',
                'ÂêéÁ´ØÂºÄÂèë': 'ÂêéÁ´ØÂ∑•Á®ãÂ∏à',
            }
            
            item.current_position = position_mapping.get(
                item.current_position, 
                item.current_position
            )
        
        # ÊäÄËÉΩÊ†áÂáÜÂåñ
        if item.skills:
            normalized_skills = []
            skill_mapping = {
                'javascript': 'JavaScript',
                'python': 'Python',
                'java': 'Java',
                'mysql': 'MySQL',
                'redis': 'Redis',
            }
            
            for skill in item.skills:
                skill_lower = skill.lower().strip()
                normalized_skill = skill_mapping.get(skill_lower, skill.strip())
                if normalized_skill and normalized_skill not in normalized_skills:
                    normalized_skills.append(normalized_skill)
            
            item.skills = normalized_skills
        
        return item

class DatabaseExportPipeline:
    """Êï∞ÊçÆÂ∫ìÂØºÂá∫ÁÆ°ÈÅì"""
    
    def __init__(self, mysql_settings):
        self.mysql_settings = mysql_settings
        self.connection = None
        
    @classmethod
    def from_crawler(cls, crawler):
        mysql_settings = crawler.settings.getdict("MYSQL_SETTINGS")
        return cls(mysql_settings)
    
    def open_spider(self, spider):
        """Áà¨Ëô´ÂêØÂä®Êó∂ËøûÊé•Êï∞ÊçÆÂ∫ì"""
        self.connection = mysql.connector.connect(**self.mysql_settings)
        spider.logger.info("Êï∞ÊçÆÂ∫ìËøûÊé•Â∑≤Âª∫Á´ã")
    
    def close_spider(self, spider):
        """Áà¨Ëô´ÁªìÊùüÊó∂ÂÖ≥Èó≠Êï∞ÊçÆÂ∫ìËøûÊé•"""
        if self.connection:
            self.connection.close()
            spider.logger.info("Êï∞ÊçÆÂ∫ìËøûÊé•Â∑≤ÂÖ≥Èó≠")
    
    def process_item(self, item: CandidateItem, spider):
        """Â§ÑÁêÜÊï∞ÊçÆÈ°πÂπ∂ÊèíÂÖ•Êï∞ÊçÆÂ∫ì"""
        try:
            cursor = self.connection.cursor()
            
            # Ê£ÄÊü•ÊòØÂê¶Â∑≤Â≠òÂú®
            if item.email:
                cursor.execute(
                    "SELECT id FROM jobs_candidate WHERE email = %s",
                    (item.email,)
                )
                if cursor.fetchone():
                    spider.logger.info(f"Ë∑≥ËøáÈáçÂ§çÈÇÆÁÆ±: {item.email}")
                    return item
            
            # ÊèíÂÖ•ÂÄôÈÄâ‰∫∫Êï∞ÊçÆ
            insert_sql = """
                INSERT INTO jobs_candidate (
                    name, email, phone, current_position, current_company,
                    experience_years, education, skills, resume_file, resume_content,
                    ai_parsed_data, predicted_job_tags, salary_expectation,
                    location_preference, availability, notes, status, source,
                    user_id, created_at, updated_at
                ) VALUES (
                    %(name)s, %(email)s, %(phone)s, %(current_position)s, %(current_company)s,
                    %(experience_years)s, %(education)s, %(skills)s, %(resume_file)s, %(resume_content)s,
                    %(ai_parsed_data)s, %(predicted_job_tags)s, %(salary_expectation)s,
                    %(location_preference)s, %(availability)s, %(notes)s, %(status)s, %(source)s,
                    %(user_id)s, %(created_at)s, %(updated_at)s
                )
            """
            
            data = item.to_dict()
            data['user_id'] = 1  # ÈªòËÆ§Áî®Êà∑IDÔºåÂèØÊ†πÊçÆÈúÄË¶ÅË∞ÉÊï¥
            
            cursor.execute(insert_sql, data)
            self.connection.commit()
            
            spider.logger.info(f"ÊàêÂäüÊèíÂÖ•ÂÄôÈÄâ‰∫∫: {item.name}")
            
        except Exception as e:
            spider.logger.error(f"ÊèíÂÖ•Êï∞ÊçÆÂ§±Ë¥•: {e}")
            self.connection.rollback()
        
        finally:
            cursor.close()
        
        return item

class JSONExportPipeline:
    """JSONÊñá‰ª∂ÂØºÂá∫ÁÆ°ÈÅì"""
    
    def __init__(self):
        self.file = None
        self.items = []
    
    def open_spider(self, spider):
        """ÊâìÂºÄËæìÂá∫Êñá‰ª∂"""
        filename = f"candidates_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.file = open(filename, 'w', encoding='utf-8')
        spider.logger.info(f"JSONÂØºÂá∫Êñá‰ª∂: {filename}")
    
    def close_spider(self, spider):
        """ÂÖ≥Èó≠Êñá‰ª∂Âπ∂ÂÜôÂÖ•Êï∞ÊçÆ"""
        if self.file:
            json.dump(self.items, self.file, ensure_ascii=False, indent=2)
            self.file.close()
            spider.logger.info(f"JSONÂØºÂá∫ÂÆåÊàêÔºåÂÖ± {len(self.items)} Êù°ËÆ∞ÂΩï")
    
    def process_item(self, item: CandidateItem, spider):
        """Êî∂ÈõÜÊï∞ÊçÆÈ°π"""
        self.items.append(item.to_dict())
        return item
```

### 3Ô∏è‚É£ **Êï∞ÊçÆÊ†ºÂºèÂåñÂô® (formatter.py)**

```python
import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime

class CandidateDataFormatter:
    """ÂÄôÈÄâ‰∫∫Êï∞ÊçÆÊ†ºÂºèÂåñÂô®"""
    
    @staticmethod
    def extract_experience_years(experience_text: str) -> Optional[int]:
        """‰ªéÊñáÊú¨‰∏≠ÊèêÂèñÂ∑•‰ΩúÂπ¥Èôê"""
        if not experience_text:
            return None
        
        # ÂåπÈÖçÂêÑÁßçÂπ¥ÈôêË°®ËææÊñπÂºè
        patterns = [
            r'(\d+)\s*Âπ¥',
            r'(\d+)\s*Âπ¥Â∑•‰ΩúÁªèÈ™å',
            r'Â∑•‰Ωú\s*(\d+)\s*Âπ¥',
            r'(\d+)\s*years?',
            r'experience:\s*(\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, experience_text, re.IGNORECASE)
            if match:
                years = int(match.group(1))
                return years if 0 <= years <= 50 else None
        
        return None
    
    @staticmethod
    def parse_skills(skills_text: str) -> List[str]:
        """Ëß£ÊûêÊäÄËÉΩÊñáÊú¨‰∏∫ÊäÄËÉΩÂàóË°®"""
        if not skills_text:
            return []
        
        # Â∏∏ËßÅÂàÜÈöîÁ¨¶
        separators = [',', 'Ôºå', ';', 'Ôºõ', '|', '/', '„ÄÅ', '\n', '\t']
        
        # ÂàÜÂâ≤ÊäÄËÉΩ
        skills = [skills_text]
        for sep in separators:
            new_skills = []
            for skill in skills:
                new_skills.extend(skill.split(sep))
            skills = new_skills
        
        # Ê∏ÖÁêÜÂíåÊ†áÂáÜÂåñ
        cleaned_skills = []
        for skill in skills:
            skill = skill.strip()
            if skill and len(skill) > 1:
                cleaned_skills.append(skill)
        
        return list(set(cleaned_skills))  # ÂéªÈáç
    
    @staticmethod
    def format_salary(salary_text: str) -> Optional[str]:
        """Ê†ºÂºèÂåñËñ™ËµÑ‰ø°ÊÅØ"""
        if not salary_text:
            return None
        
        # Ê†áÂáÜÂåñËñ™ËµÑÊ†ºÂºè
        salary = salary_text.strip()
        
        # Â∏∏ËßÅËñ™ËµÑÊ†ºÂºèÊò†Â∞Ñ
        salary_patterns = [
            (r'(\d+)[kK][-~](\d+)[kK]', r'\1K-\2K'),
            (r'(\d+)‰∏á[-~](\d+)‰∏á', r'\1‰∏á-\2‰∏á'),
            (r'Âπ¥Ëñ™(\d+)‰∏á', r'Âπ¥Ëñ™\1‰∏á'),
            (r'ÊúàËñ™(\d+)[kK]', r'ÊúàËñ™\1K'),
        ]
        
        for pattern, replacement in salary_patterns:
            salary = re.sub(pattern, replacement, salary, flags=re.IGNORECASE)
        
        return salary
    
    @staticmethod
    def generate_ai_parsed_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """ÁîüÊàêAIËß£ÊûêÊï∞ÊçÆÊ†ºÂºè"""
        
        ai_data = {
            "extraction_info": {
                "model_used": "crawler_extraction",
                "confidence": 0.8,
                "extraction_time": datetime.now().isoformat(),
                "source_type": "web_crawling"
            },
            "raw_source": {
                "url": raw_data.get('source_url'),
                "platform": raw_data.get('platform'),
                "crawl_time": datetime.now().isoformat()
            }
        }
        
        # Ê∑ªÂä†‰∏™‰∫∫‰ø°ÊÅØ
        if any(key in raw_data for key in ['name', 'email', 'phone']):
            ai_data["personal_info"] = {
                "full_name": raw_data.get('name'),
                "contact_info": {
                    "emails": [raw_data['email']] if raw_data.get('email') else [],
                    "phones": [raw_data['phone']] if raw_data.get('phone') else []
                }
            }
        
        # Ê∑ªÂä†Â∑•‰ΩúÁªèÈ™å
        if raw_data.get('current_company') or raw_data.get('current_position'):
            ai_data["work_experience"] = [{
                "company": raw_data.get('current_company'),
                "position": raw_data.get('current_position'),
                "is_current": True
            }]
        
        # Ê∑ªÂä†ÊäÄËÉΩÂàÜÊûê
        if raw_data.get('skills'):
            ai_data["skills_analysis"] = {
                "technical_skills": raw_data['skills'],
                "extraction_method": "text_parsing"
            }
        
        return ai_data

class DataExporter:
    """Êï∞ÊçÆÂØºÂá∫Âô®"""
    
    @staticmethod
    def export_to_sql(candidates: List[CandidateItem], filename: str) -> None:
        """ÂØºÂá∫‰∏∫SQLÊèíÂÖ•ËØ≠Âè•"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("-- ÂÄôÈÄâ‰∫∫Êï∞ÊçÆÂØºÂÖ•SQL\n")
            f.write("-- ÁîüÊàêÊó∂Èó¥: {}\n\n".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
            
            f.write("SET NAMES utf8mb4;\n")
            f.write("SET FOREIGN_KEY_CHECKS = 0;\n\n")
            
            for candidate in candidates:
                data = candidate.to_dict()
                
                # ÁîüÊàêINSERTËØ≠Âè•
                f.write("INSERT INTO jobs_candidate (\n")
                f.write("    name, email, phone, current_position, current_company,\n")
                f.write("    experience_years, education, skills, resume_content,\n")
                f.write("    ai_parsed_data, predicted_job_tags, salary_expectation,\n")
                f.write("    location_preference, availability, notes, status, source,\n")
                f.write("    user_id, created_at, updated_at\n")
                f.write(") VALUES (\n")
                
                # Ê†ºÂºèÂåñÂÄº
                values = []
                for key in ['name', 'email', 'phone', 'current_position', 'current_company']:
                    val = data.get(key)
                    values.append(f"'{val}'" if val else 'NULL')
                
                values.append(str(data.get('experience_years')) if data.get('experience_years') else 'NULL')
                
                for key in ['education', 'skills', 'resume_content', 'ai_parsed_data', 
                           'predicted_job_tags', 'salary_expectation', 'location_preference',
                           'availability', 'notes', 'status', 'source']:
                    val = data.get(key)
                    if val:
                        # ËΩ¨‰πâÂçïÂºïÂè∑
                        escaped_val = str(val).replace("'", "\\'")
                        values.append(f"'{escaped_val}'")
                    else:
                        values.append('NULL')
                
                values.append('1')  # user_id
                values.append(f"'{data['created_at'].strftime('%Y-%m-%d %H:%M:%S')}'")
                values.append(f"'{data['updated_at'].strftime('%Y-%m-%d %H:%M:%S')}'")
                
                f.write("    " + ",\n    ".join(values) + "\n")
                f.write(");\n\n")
            
            f.write("SET FOREIGN_KEY_CHECKS = 1;\n")
    
    @staticmethod
    def export_to_api_format(candidates: List[CandidateItem]) -> List[Dict[str, Any]]:
        """ÂØºÂá∫‰∏∫APIÊé•Âè£Ê†ºÂºè"""
        
        api_data = []
        for candidate in candidates:
            data = candidate.to_dict()
            
            # APIÊ†ºÂºèËΩ¨Êç¢
            api_item = {
                "candidate_info": {
                    "basic": {
                        "name": data['name'],
                        "email": data['email'],
                        "phone": data['phone']
                    },
                    "career": {
                        "current_position": data['current_position'],
                        "current_company": data['current_company'],
                        "experience_years": data['experience_years'],
                        "education": data['education']
                    },
                    "skills": json.loads(data['skills']) if data['skills'] else [],
                    "preferences": {
                        "salary_expectation": data['salary_expectation'],
                        "location_preference": data['location_preference'],
                        "availability": data['availability']
                    }
                },
                "metadata": {
                    "source": data['source'],
                    "status": data['status'],
                    "notes": data['notes'],
                    "created_at": data['created_at'].isoformat(),
                    "updated_at": data['updated_at'].isoformat()
                },
                "ai_data": json.loads(data['ai_parsed_data']) if data['ai_parsed_data'] else {}
            }
            
            api_data.append(api_item)
        
        return api_data
```

---

## üîå **APIÊé•Âè£ËÆæËÆ°**

### 1Ô∏è‚É£ **RESTful APIÁ´ØÁÇπ**

```python
# api/views.py
from rest_framework import viewsets, status
from rest_framework.decorators import action
from rest_framework.response import Response
from .models import CandidateData
from .serializers import CandidateSerializer

class CandidateDataViewSet(viewsets.ModelViewSet):
    """ÂÄôÈÄâ‰∫∫Êï∞ÊçÆAPIËßÜÂõæÈõÜ"""
    
    queryset = CandidateData.objects.all()
    serializer_class = CandidateSerializer
    
    @action(detail=False, methods=['post'])
    def batch_import(self, request):
        """ÊâπÈáèÂØºÂÖ•ÂÄôÈÄâ‰∫∫Êï∞ÊçÆ"""
        data = request.data.get('candidates', [])
        
        results = {
            'total': len(data),
            'success': 0,
            'failed': 0,
            'errors': []
        }
        
        for item in data:
            try:
                serializer = self.get_serializer(data=item)
                if serializer.is_valid():
                    serializer.save()
                    results['success'] += 1
                else:
                    results['failed'] += 1
                    results['errors'].append({
                        'data': item,
                        'errors': serializer.errors
                    })
            except Exception as e:
                results['failed'] += 1
                results['errors'].append({
                    'data': item,
                    'error': str(e)
                })
        
        return Response(results, status=status.HTTP_200_OK)
    
    @action(detail=False, methods=['get'])
    def export_format(self, request):
        """Ëé∑ÂèñÊï∞ÊçÆÂØºÂá∫Ê†ºÂºèËØ¥Êòé"""
        format_info = {
            'required_fields': ['name'],
            'optional_fields': [
                'email', 'phone', 'current_position', 'current_company',
                'experience_years', 'education', 'skills', 'salary_expectation',
                'location_preference', 'availability', 'notes', 'source'
            ],
            'data_formats': {
                'skills': 'JSONÊï∞ÁªÑÊàñÈÄóÂè∑ÂàÜÈöîÂ≠óÁ¨¶‰∏≤',
                'experience_years': 'Êï¥Êï∞',
                'salary_expectation': 'Â≠óÁ¨¶‰∏≤ÊèèËø∞',
                'email': 'ÊúâÊïàÈÇÆÁÆ±Ê†ºÂºè',
                'phone': '11‰ΩçÊâãÊú∫Âè∑'
            },
            'example': {
                'name': 'Âº†‰∏â',
                'email': 'zhangsan@example.com',
                'phone': '13800138000',
                'current_position': 'PythonÂ∑•Á®ãÂ∏à',
                'current_company': 'Â≠óËäÇË∑≥Âä®',
                'experience_years': 5,
                'skills': ['Python', 'Django', 'MySQL'],
                'salary_expectation': 'Âπ¥Ëñ™50-60‰∏á',
                'location_preference': 'Âåó‰∫¨',
                'availability': 'ÈöèÊó∂Âà∞Â≤ó',
                'source': 'ÁΩëÁªúÁà¨Ëô´'
            }
        }
        
        return Response(format_info)
```

### 2Ô∏è‚É£ **Êï∞ÊçÆÂ∫èÂàóÂåñÂô®**

```python
# api/serializers.py
from rest_framework import serializers
from .models import CandidateData
import json

class CandidateSerializer(serializers.ModelSerializer):
    """ÂÄôÈÄâ‰∫∫Êï∞ÊçÆÂ∫èÂàóÂåñÂô®"""
    
    skills_list = serializers.ListField(
        child=serializers.CharField(),
        write_only=True,
        required=False,
        help_text="ÊäÄËÉΩÂàóË°®"
    )
    
    class Meta:
        model = CandidateData
        fields = '__all__'
        extra_kwargs = {
            'name': {'required': True, 'help_text': 'ÂÄôÈÄâ‰∫∫ÂßìÂêç'},
            'email': {'required': False, 'help_text': 'ÈÇÆÁÆ±Âú∞ÂùÄ'},
            'phone': {'required': False, 'help_text': 'ÊâãÊú∫Âè∑Á†Å'},
            'skills': {'read_only': True},
        }
    
    def validate_email(self, value):
        """È™åËØÅÈÇÆÁÆ±Ê†ºÂºè"""
        if value:
            import re
            pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            if not re.match(pattern, value):
                raise serializers.ValidationError("ÈÇÆÁÆ±Ê†ºÂºè‰∏çÊ≠£Á°Æ")
        return value
    
    def validate_phone(self, value):
        """È™åËØÅÊâãÊú∫Âè∑Ê†ºÂºè"""
        if value:
            import re
            phone_clean = re.sub(r'[^\d]', '', value)
            if not (len(phone_clean) == 11 and phone_clean.startswith('1')):
                raise serializers.ValidationError("ÊâãÊú∫Âè∑Ê†ºÂºè‰∏çÊ≠£Á°Æ")
            return phone_clean
        return value
    
    def validate_experience_years(self, value):
        """È™åËØÅÂ∑•‰ΩúÂπ¥Èôê"""
        if value is not None:
            if not isinstance(value, int) or value < 0 or value > 50:
                raise serializers.ValidationError("Â∑•‰ΩúÂπ¥ÈôêÂøÖÈ°ªÊòØ0-50‰πãÈó¥ÁöÑÊï¥Êï∞")
        return value
    
    def create(self, validated_data):
        """ÂàõÂª∫ÂÄôÈÄâ‰∫∫ËÆ∞ÂΩï"""
        skills_list = validated_data.pop('skills_list', [])
        
        # Â§ÑÁêÜÊäÄËÉΩÂàóË°®
        if skills_list:
            validated_data['skills'] = json.dumps(skills_list, ensure_ascii=False)
        
        return super().create(validated_data)
    
    def to_representation(self, instance):
        """Ëá™ÂÆö‰πâËæìÂá∫Ê†ºÂºè"""
        data = super().to_representation(instance)
        
        # Ëß£ÊûêÊäÄËÉΩJSON
        if data.get('skills'):
            try:
                data['skills_list'] = json.loads(data['skills'])
            except json.JSONDecodeError:
                data['skills_list'] = []
        
        # Ëß£ÊûêAIÊï∞ÊçÆ
        if data.get('ai_parsed_data'):
            try:
                data['ai_parsed_data'] = json.loads(data['ai_parsed_data'])
            except json.JSONDecodeError:
                data['ai_parsed_data'] = {}
        
        return data
```

---

## üìö **‰ΩøÁî®ÊñáÊ°£**

### 1Ô∏è‚É£ **Âø´ÈÄüÂºÄÂßã**

```bash
# 1. ÂàõÂª∫È°πÁõÆ
mkdir candidate_crawler
cd candidate_crawler

# 2. ÂàõÂª∫ËôöÊãüÁéØÂ¢É
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# 3. ÂÆâË£Ö‰æùËµñ
pip install scrapy django djangorestframework mysql-connector-python

# 4. ÂàõÂª∫DjangoÈ°πÁõÆ
django-admin startproject api_server
cd api_server
python manage.py startapp candidates

# 5. ÂàõÂª∫ScrapyÈ°πÁõÆ
cd ..
scrapy startproject crawler
```

### 2Ô∏è‚É£ **ÈÖçÁΩÆÁ§∫‰æã**

```python
# settings.py (Scrapy)
ITEM_PIPELINES = {
    'crawler.pipelines.DataValidationPipeline': 100,
    'crawler.pipelines.DataNormalizationPipeline': 200,
    'crawler.pipelines.JSONExportPipeline': 300,
    'crawler.pipelines.DatabaseExportPipeline': 400,
}

MYSQL_SETTINGS = {
    'host': 'localhost',
    'port': 3306,
    'user': 'headhunter_user',
    'password': 'your_password',
    'database': 'headhunter_db',
    'charset': 'utf8mb4'
}

# Django settings.py
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'headhunter_db',
        'USER': 'headhunter_user',
        'PASSWORD': 'your_password',
        'HOST': 'localhost',
        'PORT': '3306',
    }
}

REST_FRAMEWORK = {
    'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination',
    'PAGE_SIZE': 50
}
```

### 3Ô∏è‚É£ **APIË∞ÉÁî®Á§∫‰æã**

```python
import requests
import json

# ÊâπÈáèÂØºÂÖ•ÂÄôÈÄâ‰∫∫Êï∞ÊçÆ
candidates_data = [
    {
        "name": "Âº†‰∏â",
        "email": "zhangsan@example.com",
        "phone": "13800138000",
        "current_position": "PythonÂ∑•Á®ãÂ∏à",
        "current_company": "Â≠óËäÇË∑≥Âä®",
        "experience_years": 5,
        "skills_list": ["Python", "Django", "MySQL", "Redis"],
        "salary_expectation": "Âπ¥Ëñ™50-60‰∏á",
        "location_preference": "Âåó‰∫¨",
        "source": "ÁΩëÁªúÁà¨Ëô´"
    }
]

response = requests.post(
    'http://localhost:8000/api/candidates/batch_import/',
    json={'candidates': candidates_data},
    headers={'Content-Type': 'application/json'}
)

print(response.json())
```

---

## üöÄ **È°πÁõÆÊé®ËçêÂ∑•ÂÖ∑ÂíåÊ°ÜÊû∂**

### üì¶ **‰æùËµñÂåÖÊé®Ëçê**

```txt
# requirements.txt
# Áà¨Ëô´Ê°ÜÊû∂
scrapy==2.11.0
scrapy-splash==0.8.0

# WebÊ°ÜÊû∂  
django==4.2.7
djangorestframework==3.14.0

# Êï∞ÊçÆÂ∫ì
mysql-connector-python==8.2.0
PyMySQL==1.1.0

# Êï∞ÊçÆÂ§ÑÁêÜ
pandas==2.1.3
numpy==1.24.3

# ÊñáÊú¨Â§ÑÁêÜ
jieba==0.42.1
python-docx==1.1.0
PyPDF2==3.0.1

# Â∑•ÂÖ∑Â∫ì
requests==2.31.0
lxml==4.9.3
beautifulsoup4==4.12.2
fake-useragent==1.4.0

# ÂºÇÊ≠•ÊîØÊåÅ
aiohttp==3.9.1
asyncio==3.4.3

# Êó•ÂøóÂíåÁõëÊéß
loguru==0.7.2
```

---

## üìä **Â≠óÊÆµÊò†Â∞ÑÂØπÁÖßË°®**

| ÁõÆÊ†áÂ≠óÊÆµ | Êï∞ÊçÆÁ±ªÂûã | ËØ¥Êòé | Á§∫‰æã |
|---------|---------|------|------|
| `name` | varchar(100) | ÂÄôÈÄâ‰∫∫ÂßìÂêçÔºåÂøÖÂ°´ | "Âº†‰∏â" |
| `email` | varchar(254) | ÈÇÆÁÆ±Âú∞ÂùÄÔºåÁî®‰∫éÂéªÈáç | "zhangsan@example.com" |
| `phone` | varchar(20) | ÊâãÊú∫Âè∑Á†ÅÔºå11‰ΩçÊï∞Â≠ó | "13800138000" |
| `current_position` | varchar(200) | ÂΩìÂâçËÅå‰Ωç | "È´òÁ∫ßPythonÂ∑•Á®ãÂ∏à" |
| `current_company` | varchar(200) | ÂΩìÂâçÂÖ¨Âè∏ | "Â≠óËäÇË∑≥Âä®" |
| `experience_years` | int(11) | Â∑•‰ΩúÂπ¥ÈôêÔºåÊï¥Êï∞ | 5 |
| `education` | varchar(200) | ÊúÄÈ´òÂ≠¶ÂéÜ | "Êú¨Áßë" |
| `skills` | longtext | ÊäÄËÉΩÊ†áÁ≠æÔºåJSONÊï∞ÁªÑÊ†ºÂºè | '["Python", "Django"]' |
| `predicted_job_tags` | longtext | AIÈ¢ÑÊµãËÅå‰ΩçÊ†áÁ≠æÔºåJSON | '["ÂêéÁ´ØÂ∑•Á®ãÂ∏à", "ÂÖ®Ê†àÂ∑•Á®ãÂ∏à"]' |
| `resume_content` | longtext | ÁÆÄÂéÜÊñáÊú¨ÂÜÖÂÆπ | "‰∏™‰∫∫ÁÆÄÂéÜÂÜÖÂÆπ..." |
| `ai_parsed_data` | longtext | AIËß£ÊûêÁªìÊûÑÂåñÊï∞ÊçÆÔºåJSON | '{"personal_info": {...}}' |
| `salary_expectation` | varchar(100) | ÊúüÊúõËñ™ËµÑ | "Âπ¥Ëñ™50-60‰∏á" |
| `location_preference` | varchar(200) | ÊúüÊúõÂ∑•‰ΩúÂú∞ÁÇπ | "Âåó‰∫¨" |
| `availability` | varchar(50) | ÂèØÂÖ•ËÅåÊó∂Èó¥ | "ÈöèÊó∂Âà∞Â≤ó" |
| `notes` | longtext | Â§áÊ≥®‰ø°ÊÅØ | "‰ªéXXÁΩëÁ´ôÁà¨Âèñ" |
| `status` | varchar(20) | Áä∂ÊÄÅ | "active" |
| `source` | varchar(100) | Êï∞ÊçÆÊù•Ê∫êÊ∏†ÈÅì | "ÁΩëÁªúÁà¨Ëô´" |
| `user_id` | bigint(20) | ÊâÄÂ±ûÁî®Êà∑ID | 1 |
| `created_at` | datetime(6) | ÂàõÂª∫Êó∂Èó¥ | "2024-01-15 10:30:00" |
| `updated_at` | datetime(6) | Êõ¥Êñ∞Êó∂Èó¥ | "2024-01-15 10:30:00" |

---

## üéØ **È°πÁõÆÂÆûÊñΩÂª∫ËÆÆ**

### 1Ô∏è‚É£ **ÂºÄÂèëÈò∂ÊÆµËßÑÂàí**

1. **Èò∂ÊÆµ1: Âü∫Á°ÄÊ°ÜÊû∂Êê≠Âª∫**
   - ÂàõÂª∫ScrapyÂíåDjangoÈ°πÁõÆ
   - ÈÖçÁΩÆÊï∞ÊçÆÂ∫ìËøûÊé•
   - ÂÆûÁé∞Âü∫Á°ÄÊï∞ÊçÆÊ®°Âûã

2. **Èò∂ÊÆµ2: Áà¨Ëô´ÂºÄÂèë**
   - ÁºñÂÜôÁõÆÊ†áÁΩëÁ´ôÁà¨Ëô´
   - ÂÆûÁé∞Êï∞ÊçÆÊèêÂèñÈÄªËæë
   - Ê∑ªÂä†Êï∞ÊçÆÈ™åËØÅÂíåÊ∏ÖÊ¥ó

3. **Èò∂ÊÆµ3: APIÊé•Âè£ÂºÄÂèë**
   - ÂÆûÁé∞RESTful API
   - Ê∑ªÂä†ÊâπÈáèÂØºÂÖ•ÂäüËÉΩ
   - ÂÆåÂñÑÊï∞ÊçÆÂ∫èÂàóÂåñ

4. **Èò∂ÊÆµ4: Êï∞ÊçÆÂ§ÑÁêÜ‰ºòÂåñ**
   - ‰ºòÂåñÊï∞ÊçÆÊ†ºÂºèÂåñÈÄªËæë
   - Ê∑ªÂä†ÈîôËØØÂ§ÑÁêÜÊú∫Âà∂
   - ÂÆûÁé∞Êï∞ÊçÆÂØºÂá∫ÂäüËÉΩ

### 2Ô∏è‚É£ **ÊäÄÊúØÈÄâÂûãÂª∫ËÆÆ**

- **Áà¨Ëô´Ê°ÜÊû∂**: Scrapy (ÂäüËÉΩÂÆåÂñÑÔºåÊâ©Â±ïÊÄßÂ•Ω)
- **WebÊ°ÜÊû∂**: Django + DRF (Âø´ÈÄüÂºÄÂèëÔºåAPIÂèãÂ•Ω)
- **Êï∞ÊçÆÂ∫ì**: MySQL (‰∏éÁõÆÊ†áÁ≥ªÁªüÂÖºÂÆπ)
- **Êï∞ÊçÆÂ§ÑÁêÜ**: Pandas (Êï∞ÊçÆÂàÜÊûêÂíåÊ∏ÖÊ¥ó)
- **‰ªªÂä°ÈòüÂàó**: Celery (ÂèØÈÄâÔºåÂ§ÑÁêÜÂ§ßÊâπÈáèÊï∞ÊçÆ)

### 3Ô∏è‚É£ **ÈÉ®ÁΩ≤Âª∫ËÆÆ**

- **ÂºÄÂèëÁéØÂ¢É**: ‰ΩøÁî®ËôöÊãüÁéØÂ¢ÉÈöîÁ¶ª‰æùËµñ
- **ÊµãËØïÁéØÂ¢É**: Â∞èÊâπÈáèÊï∞ÊçÆÊµãËØï
- **Áîü‰∫ßÁéØÂ¢É**: DockerÂÆπÂô®ÂåñÈÉ®ÁΩ≤
- **ÁõëÊéß**: Ê∑ªÂä†Êó•ÂøóÂíåÊÄßËÉΩÁõëÊéß

Ëøô‰∏™ÊñáÊ°£Êèê‰æõ‰∫ÜÂÆåÊï¥ÁöÑÊï∞ÊçÆÁªìÊûÑ„ÄÅ‰ª£Á†ÅÊ°ÜÊû∂ÂíåÂÆûÊñΩÊåáÂçóÔºåÂèØ‰ª•‰Ωú‰∏∫Áà¨Ëô´È°πÁõÆÂºÄÂèëÁöÑÊäÄÊúØÂèÇËÄÉÊñáÊ°£ÔºÅ üöÄ